{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plain_Transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WgCRT5isMVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/pytorch/fairseq.git\n",
        "\n",
        "cd fairseq\n",
        "pip install fastBPE regex requests sacremoses subword_nmt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij9cdrZysSL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "cd fairseq/\n",
        "\n",
        "pip install --editable ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBN7bse0szhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEPv_6B9sfFe",
        "colab_type": "code",
        "outputId": "306a18c5-7b6b-4d19-d9b6-783ec463d8cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "\n",
        "\"\"\" \n",
        "Tokenize Hi-En parallel corpus\n",
        "Data present in 'fairseq/hi-en/pruned_train.hi' and 'fairseq/hi-en/pruned_train.en'\n",
        "Adapted from https://github.com/facebookresearch/MIXER/blob/master/prepareData.sh\n",
        "\"\"\"\n",
        "%%bash\n",
        "cd fairseq/\n",
        "\n",
        "echo 'Cloning Moses github repository (for tokenization scripts)...'\n",
        "git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "\n",
        "echo 'Cloning Subword NMT repository (for BPE pre-processing)...'\n",
        "git clone https://github.com/rsennrich/subword-nmt.git\n",
        "\n",
        "SCRIPTS=mosesdecoder/scripts\n",
        "TOKENIZER=$SCRIPTS/tokenizer/tokenizer.perl\n",
        "CLEAN=$SCRIPTS/training/clean-corpus-n.perl\n",
        "NORM_PUNC=$SCRIPTS/tokenizer/normalize-punctuation.perl\n",
        "REM_NON_PRINT_CHAR=$SCRIPTS/tokenizer/remove-non-printing-char.perl\n",
        "BPEROOT=subword-nmt/subword_nmt\n",
        "BPE_TOKENS=20000\n",
        "\n",
        "CORPORA=('pruned_train')\n",
        "\n",
        "if [ ! -d \"$SCRIPTS\" ]; then\n",
        "    echo \"Please set SCRIPTS variable correctly to point to Moses scripts.\"\n",
        "    exit\n",
        "fi\n",
        "\n",
        "src=hi\n",
        "tgt=en\n",
        "lang=hi-en\n",
        "prep=iitb_hi_en\n",
        "tmp=$prep/tmp\n",
        "orig=orig\n",
        "\n",
        "mkdir -p $orig $tmp $prep\n",
        "\n",
        "\n",
        "cp hi-en/* $orig/\n",
        "\n",
        "echo \"pre-processing train data...\"\n",
        "for l in $src $tgt; do\n",
        "    for f in \"${CORPORA[@]}\"; do\n",
        "        cat $orig/$f.$l | \\\n",
        "            perl $NORM_PUNC $l | \\\n",
        "            perl $REM_NON_PRINT_CHAR | \\\n",
        "            perl $TOKENIZER -threads 8 -a -l $l >> $tmp/train.tags.$lang.tok.$l\n",
        "    done\n",
        "done\n",
        "\n",
        "\n",
        "echo \"splitting train and valid...\"\n",
        "for l in $src $tgt; do\n",
        "    awk '{if (NR%1333 == 0)  print $0;}' $tmp/train.tags.$lang.tok.$l > $tmp/valid.$l\n",
        "    awk '{if (NR%1333 != 0)  print $0;}' $tmp/train.tags.$lang.tok.$l > $tmp/train.$l\n",
        "done\n",
        "\n",
        "\n",
        "TRAIN=$tmp/train.hi-en\n",
        "BPE_CODE=$prep/code\n",
        "rm -f $TRAIN\n",
        "for l in $src $tgt; do\n",
        "    cat $tmp/train.$l >> $TRAIN\n",
        "done\n",
        "\n",
        "echo \"learn_bpe.py on ${TRAIN}...\"\n",
        "python $BPEROOT/learn_bpe.py -s $BPE_TOKENS < $TRAIN > $BPE_CODE\n",
        "\n",
        "for L in $src $tgt; do\n",
        "    for f in train.$L valid.$L; do\n",
        "        echo \"apply_bpe.py to ${f}...\"\n",
        "        python $BPEROOT/apply_bpe.py -c $BPE_CODE < $tmp/$f > $tmp/bpe.$f\n",
        "    done\n",
        "done\n",
        "\n",
        "perl $CLEAN -ratio 1.5 $tmp/bpe.train $src $tgt $prep/train 1 250\n",
        "perl $CLEAN -ratio 1.5 $tmp/bpe.valid $src $tgt $prep/valid 1 250\n",
        "\n",
        "echo 'Done'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning Moses github repository (for tokenization scripts)...\n",
            "Cloning Subword NMT repository (for BPE pre-processing)...\n",
            "pre-processing train data...\n",
            "splitting train and valid...\n",
            "learn_bpe.py on iitb_hi_en/tmp/train.hi-en...\n",
            "apply_bpe.py to train.hi...\n",
            "apply_bpe.py to valid.hi...\n",
            "apply_bpe.py to train.en...\n",
            "apply_bpe.py to valid.en...\n",
            "Done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mosesdecoder'...\n",
            "Cloning into 'subword-nmt'...\n",
            "Tokenizer Version 1.1\n",
            "Language: hi\n",
            "Number of threads: 8\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 8\n",
            "clean-corpus.perl: processing iitb_hi_en/tmp/bpe.train.hi & .en to iitb_hi_en/train, cutoff 1-250, ratio 1.5\n",
            "..........(100000)..........(200000)..........(300000)..........(400000)..........(500000)..........(600000)..........(700000)........\n",
            "Input sentences: 787507  Output sentences:  479248\n",
            "clean-corpus.perl: processing iitb_hi_en/tmp/bpe.valid.hi & .en to iitb_hi_en/valid, cutoff 1-250, ratio 1.5\n",
            "\n",
            "Input sentences: 591  Output sentences:  360\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEq3SWVYswah",
        "colab_type": "code",
        "outputId": "2fc80082-99e4-479c-b52a-ccebcab6b028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "%%bash\n",
        "cd fairseq/\n",
        "\n",
        "TEXT=iitb_hi_en\n",
        "fairseq-preprocess --source-lang hi --target-lang en \\\n",
        "    --trainpref $TEXT/train --validpref $TEXT/valid \\\n",
        "    --destdir data-bin/iitb_hi_en"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-30 20:16:22 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bpe=None, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/iitb_hi_en', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, quantization_config_path=None, seed=1, source_lang='hi', srcdict=None, target_lang='en', task='translation', tensorboard_logdir='', testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='iitb_hi_en/train', user_dir=None, validpref='iitb_hi_en/valid', workers=1)\n",
            "2020-04-30 20:17:59 | INFO | fairseq_cli.preprocess | [hi] Dictionary: 11871 types\n",
            "2020-04-30 20:19:43 | INFO | fairseq_cli.preprocess | [hi] iitb_hi_en/train.hi: 479248 sents, 15225681 tokens, 0.0% replaced by <unk>\n",
            "2020-04-30 20:19:43 | INFO | fairseq_cli.preprocess | [hi] Dictionary: 11871 types\n",
            "2020-04-30 20:19:43 | INFO | fairseq_cli.preprocess | [hi] iitb_hi_en/valid.hi: 360 sents, 11037 tokens, 0.0% replaced by <unk>\n",
            "2020-04-30 20:19:43 | INFO | fairseq_cli.preprocess | [en] Dictionary: 12463 types\n",
            "2020-04-30 20:21:08 | INFO | fairseq_cli.preprocess | [en] iitb_hi_en/train.en: 479248 sents, 12585605 tokens, 0.0% replaced by <unk>\n",
            "2020-04-30 20:21:08 | INFO | fairseq_cli.preprocess | [en] Dictionary: 12463 types\n",
            "2020-04-30 20:21:08 | INFO | fairseq_cli.preprocess | [en] iitb_hi_en/valid.en: 360 sents, 9135 tokens, 0.0219% replaced by <unk>\n",
            "2020-04-30 20:21:08 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/iitb_hi_en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEUNTucpwMDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Hi-En fairseq model\n",
        "!mkdir hi-en-best/\n",
        "!cd fairseq/ && python train.py data-bin/iitb_hi_en --label-smoothing 0.1 --adam-betas '(0.9,0.98)' \\\n",
        "    --optimizer adam -s hi -t en --criterion label_smoothed_cross_entropy --lr 0.0005 --lr-scheduler inverse_sqrt --clip-norm 0.1 --dropout 0.2 --max-tokens 2000 \\\n",
        "    --arch transformer_vaswani_wmt_en_fr_big --save-dir /content/gdrive/My\\ Drive/hi-en-best --max-epoch 12 | tee -a ../hi-en-best/training.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q17jGKAtwhry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "bf0205e0-9e25-4b38-d878-dd603b4d5793"
      },
      "source": [
        "# Restart runtime if en-fr load error persists\n",
        "pip install fairseq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fairseq in ./fairseq (0.9.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.4.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.38.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq) (3.6.6)\n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq) (0.996.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq) (1.7.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE_tcdrpqjQS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cd543ae7-b4a6-45e3-a407-bffdc942429f"
      },
      "source": [
        "# Load both Hi-En model and pre-trained En-Fr model\n",
        "import os\n",
        "import torch\n",
        "from fairseq.models.transformer import TransformerModel\n",
        "from fairseq.models import FairseqEncoderDecoderModel\n",
        "os.chdir('/content/')\n",
        "hi2en = TransformerModel.from_pretrained(\n",
        "  '/content/gdrive/My Drive/hi-en-best/',\n",
        "  checkpoint_file='checkpoint_last.pt',\n",
        "  data_name_or_path='/content/fairseq/data-bin/iitb_hi_en',\n",
        "  bpe='subword_nmt',\n",
        "  bpe_codes='/content/fairseq/iitb_hi_en/code')\n",
        "\n",
        "\n",
        "en2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/fairseq/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "100%|██████████| 2316140317/2316140317 [01:09<00:00, 33486363.57B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAs7r-h-sEkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from fairseq.hub_utils import GeneratorHubInterface\n",
        "from fairseq.tasks.translation import TranslationTask\n",
        "\n",
        "hi2en_encoder = list(hi2en.models[0].children())[0]\n",
        "en2fr_decoder = list(en2fr.models[0].children())[1]\n",
        "\n",
        "class hi_fr_translator(FairseqEncoderDecoderModel):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__(encoder, decoder)\n",
        "\n",
        "class hi_fr_task(TranslationTask):\n",
        "  def __init__(self, args, src_dict, tgt_dict):\n",
        "    super().__init__(args, src_dict, tgt_dict)\n",
        "\n",
        "hi2fr_task = hi_fr_task(en2fr.args, hi2en.task.source_dictionary, en2fr.task.target_dictionary)\n",
        "\n",
        "hi2fr = hi_fr_translator(hi2en_encoder, en2fr_decoder)\n",
        "gen_obj = GeneratorHubInterface(en2fr.args, hi2fr_task, [hi2fr])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVc8SLe2sHvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate translation for test set\n",
        "test_file =  open('/content/gdrive/My Drive/hi-en-best/Tatoeba.fr-hi.hi', 'r')\n",
        " \n",
        "with open('/content/gdrive/My Drive/hi-en-best/test_results.txt', 'w') as out_file:\n",
        "  for test_line in test_file.readlines():\n",
        "    fr = gen_obj.translate(test_line, beam=3)\n",
        "    out_file.write(fr+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}